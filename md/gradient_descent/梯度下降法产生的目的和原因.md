### 使用梯度下降法目的和原因

#### 目的

梯度下降法(Gradient Descent)是一个算法，但不是像多元线性回归那样是一个具体做回归任务的算法，而是一个非常通用的优化算法来帮助一些机器学习算法求解出最优解的，所谓的通用就是很多机器学习算法都是用它，甚至深度学习也是用它来求解最优解。所有优化算法的目的都是期望以最快的速度把模型参数θ求解出来，梯度下降法就是一种经典常用的优化算法。

#### 原因

之前利用θ的解析解公式求解出来的解我们就直接说是最优解的一个原因是因为MSE这个损失函数是凸函数，但是如果我们机器学习的损失函数是非凸函数的话，设置梯度为0会得到很多个极值，甚至是极大值都有可能。

之前利用θ的解析解公式求解的另一个原因是特征维度并不多，但是细致分析一下公式里面![img](https://www.itbaizhan.com/wiki/imgs/wps594.png)对称阵是N维乘以N维的，复杂度是是O(N)的三次方，换句话说，就是如果你的特征数量翻倍，你的计算时间大致上要2的三次方，8倍的慢

比如，4个特征1秒，8个特征就是8秒，16个特征就是64秒，当维度更多的时候呢？

所以其实之前一步求出最优解并不是机器学习甚至深度学习常用的手段，如下图，之前我们是设置梯度为0，反过来求解最低点的时候θ是多少，而梯度下降法是一点点去逼近最优解！

### 梯度下降法的思想

![img](https://www.itbaizhan.com/wiki/imgs/wps595.jpg)

其实这就跟生活中的情形很像，比如你问一个朋友的工资是多少，他说你猜？然后你会觉得很难猜，他说你猜完我告诉你是猜高了还是猜低了，这样你就可以奔着对的方向一直猜下去，最后总有一下你能猜对。梯度下降法就是这样的，你得去试很多次，而且是不是我们在试的过程中还得想办法知道是不是在猜对的路上，说白了就是得到正确的反馈再调整然后继续猜才有意义。

一般你玩儿这样的游戏的时候，一开始第一下都是随机瞎猜一个对吧，那对于计算机来说是不是就是随机取值，也就是说你有![img](https://www.itbaizhan.com/wiki/imgs/wps596.png)，这里θ强调一下不是一个值，而是一个向量就是一组W，一开始的时候我们通过随机把每个值都给它随机出来。有了θ我们可以去根据算法就是公式去计算出来![img](https://www.itbaizhan.com/wiki/imgs/wps597.png)，比如![img](https://www.itbaizhan.com/wiki/imgs/wps598.png)，然后根据计算![img](https://www.itbaizhan.com/wiki/imgs/wps599.png)和真实y之间的损失比如MSE，然后调整θ再去计算MSE。

这个调整正如咱们前面说的肯定不是瞎调整，当然这个调整的方式很多，你可以整体θ每个值调大一点，也可以整体θ每个值调小一点，也可以一部分调大一部分调小。第一次![img](https://www.itbaizhan.com/wiki/imgs/wps600.png) 我们可以得到第一次的MSE就是Loss0，调整后第二次![img](https://www.itbaizhan.com/wiki/imgs/wps601.png)对应可以得到第二次的MSE就是Loss1，如果loss变小是不是调对了，就应该继续调，如果loss反而变大是不是调反了，就应该反过来调。直到MSE我们找到最小值时计算出来的![img](https://www.itbaizhan.com/wiki/imgs/wps602.png)就是我们的最优解。

这个就好比道士下山，我们把loss看出是曲线就是山谷，如果走过来就再往回调，所以是一个迭代的过程。

> θ是一组向量 有了θ我们可以计算出**y_hat**(y_hat = Xθ)，根据y_hat计算y_hat和真实y之间的损失MSE(mean squared error mes = (Xθ-y)2)，然后再调整θ再去计算MSE