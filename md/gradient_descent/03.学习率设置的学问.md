### 学习率设置的学问

![img](https://www.itbaizhan.com/wiki/imgs/wps611.png)![img](https://www.itbaizhan.com/wiki/imgs/wps612.png)

根据我们上面讲的梯度下降法公式，我们知道η是学习率，设置大的学习率Wj每次调整的幅度就大，设置小的学习率Wj每次调整的幅度就小，然而如果步子迈的太大也会有问题其实，俗话说步子大了容易扯着蛋，可能一下子迈到山另一头去了，然后一步又迈回来了，使得来来回回震荡。步子太小呢就一步步往前挪，也会使得整体迭代次数增加。

![image-20230706172604266](https://www.itbaizhan.com/wiki/imgs/image-20230706172604266.png)

![img](https://www.itbaizhan.com/wiki/imgs/wps615.jpg)

学习率的设置是门学问，一般我们会把它设置成一个比较小的正整数，0.1、0.01、0.001、0.0001，都是常见的设定数值，一般情况下学习率在整体迭代过程中是一直不变的数，但是也可以设置成随着迭代次数增多学习率逐渐变小，因为越靠近山谷我们就可以步子迈小点，省得走过，还有一些深度学习的优化算法会自己控制调整学习率这个值，后面学习过程中这些策略在讲解代码中我们会一一讲到。

![img](https://www.itbaizhan.com/wiki/imgs/wps616.jpg)

#### 全局最优解

![img](https://www.itbaizhan.com/wiki/imgs/wps617.jpg)

上图可以看出如果损失函数是非凸函数，梯度下降法是有可能落到局部最小值的，所以其实步长不能设置的太小太稳健，那样就很容易落入局部最优解，虽说局部最小值也没大问题，因为模型只要是堪用的就好嘛，但是我们肯定还是尽量要奔着全局最优解去。

 