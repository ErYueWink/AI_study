#### 全量梯度下降

Batch Gradient Descent

![img](https://www.itbaizhan.com/wiki/imgs/wps637.png)

在梯度下降中，对于θ的更新，所有的样本都有贡献，也就是参与调整θ。其计算得到的是一个标准梯度。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦。全量梯度下降每次学习都使用整个训练集，因此其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。

![img](https://www.itbaizhan.com/wiki/imgs/wps638.jpg)