### 梯度下降法公式

这里梯度下降法的公式就是一个式子指导计算机迭代过程中如何去调整θ，不需要推导和证明，就是总结出来的

![img](https://www.itbaizhan.com/wiki/imgs/wps603.png)![img](https://www.itbaizhan.com/wiki/imgs/wps604.png)

这里的Wj就是θ中的某一个j=0…n，这里的η就是图里的learning step，很多时候也叫学习率learning rate，很多时候也用α表示，这个学习率我们可以看作是下山迈的步子的大小，步子迈的大下山就快。

![img](https://www.itbaizhan.com/wiki/imgs/wps605.jpg)

> gradient为正θ向左调整 gradient为负θ向右调整

学习率一般都是正数，那么在山左侧梯度是负的，那么这个负号就会把W往大了调，如果在山右侧梯度就是正的，那么负号就会把W往小了调。每次Wj调整的幅度就是η*gradient，就是横轴上移动的距离。

如果特征或维度越多，那么这个公式用的次数就越多，也就是每次迭代要应用的这个式子n+1次，所以其实上面的图不是特别准，因为θ对应的是很多维度，应该每一个维度都可以画一个这样的图，或者是一个多维空间的图。

![img](https://www.itbaizhan.com/wiki/imgs/wps790.png)

![image-20230706172439385](https://www.itbaizhan.com/wiki/imgs/image-20230706172439385.png)

所以观察图我们可以发现不是某一个![img](https://www.itbaizhan.com/wiki/imgs/wps609.png)或![img](https://www.itbaizhan.com/wiki/imgs/wps610.png)找到最小值就是最优解，而是它们一起找到J最小的时候才是最优解。

 