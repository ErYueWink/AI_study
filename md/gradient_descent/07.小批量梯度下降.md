#### 小批量梯度下降

Mini-Batch Gradient Descent

![img](https://www.itbaizhan.com/wiki/imgs/wps641.png)

Mini-batch梯度下降综合了batch梯度下降与stochastic梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择batch_size，batch_size < m个样本进行学习。相对于随机梯度下降算法，小批量梯度下降算法降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。

![img](https://www.itbaizhan.com/wiki/imgs/wps642.jpg)

