### 轮次和批次

轮次：epoch，轮次顾名思义是把我们已有的训练集数据学习多少轮。

批次：batch，批次这里指的的我们已有的训练集数据比较多的时候，一轮要学习太多数据，那就把一轮次要学习的数据分成多个批次，一批一批数据的学习。

举个例子，这就好比有一本唐诗300首需要大家背诵，如果要给你一周时间要你背诵完，或许你很聪明可以背完，但是估计也不敢说一下子全都记得特别牢，甚至到可以出口成章的地步吧。那通常人是怎么做的呢？是不是就是接下来多花几周，重复之前一周的动作，把300首唐诗多背几次，比如花10周把300首唐诗全部倒背如流了，然后又花了10周时间把300首唐诗又背诵到了滚瓜烂熟的地步终于可以出口成唐诗了。那么刚提到的10周、又10周放在AI领域那么就是所谓的训练了10个轮次又10个轮次，总共20个轮次。

再回顾上面一段话，我们假设你很聪明，也就是AI训练的电脑性能处理能力非常好，如果是普通人或者一般的电脑，很有可能一周都不可能背诵完300首，也就是内存一下子装不下那么大的数据量，或者处理器计算能力没有那么快。当数据量大的情况下，这是很常见的现在，那么为了可以顺利背下来300首唐诗到举一反三出口成章的地方，也就是为了训练出来model模型，我们只能把一个轮次需要的数据分成多个批次来一点点计算，放到背唐诗的例子中，说白了就是一周背不下来300首唐诗，那么我们就一周背50首唐诗吧比如说，这样我们就需要6个批次把一轮数据背完，一个批次所需要的数据batch_size就是50。

还有就是对于三种梯度下降来说，全量梯度下降就是每一轮次用到全量的数据，然后一次迭代就是一个轮次，然后用全量数据计算梯度来更新一下W。随机梯度下降每一个轮次也需要计算所有数据，但是有多少数据就会分为多少个批次，即是一个批次一次迭代，就只用一条数据计算梯度来更新一下W，所以随机梯度下降一个轮次中的更新W的次数等于样本总数。最后就是mini-batch梯度下降，每一个轮次也需要计算所有数据，但是轮次分成多少个批次取决于batch_size，batch_size大需要的轮次就少，比如batch_size=num_samples，那就等价于全量批量梯度下降，batch_size=1那就等价于随机梯度下降。

num_batches=math.ceil(num_samples/batch_size)

 