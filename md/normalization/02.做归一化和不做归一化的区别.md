提到归一化，还是少不了梯度下降，如果维度多了，就是超平面，很难去画出来了，所以我们选择只有两个维度的情况，那么我们可以把损失函数看作是山峰山谷，如果拿多元线性回归举例的话，因为多元线性回归的损失函数MSE是凸函数，所以我们可以把损失函数看成是一个碗。然后下面的图就是从碗上方去俯瞰！哪里是损失最小的地方呢？当然对应的就是碗底的地方！所以下图碗中心的地方颜色较浅的区域就是cost损失较小的地方。

下图左是做了归一化的俯瞰图，下图右是没有做归一化的俯瞰图。

![img](https://www.itbaizhan.com/wiki/imgs/wps652.jpg)

我们先来说一下为什么没做归一化是右侧图示，举个例子假如我们有一人客户信息，然后里面有两个维度，一个是用户的年龄，一个是用户的月收入，不管目标变量是什么多元线性回归的式子我们可以里面写出来，不考虑截距项![img](https://www.itbaizhan.com/wiki/imgs/wps653.png)，那么这样每一条样本的不同维度对应的数量级不同，原因是每个维度对应的物理含义不同嘛，但是计算机能理解这25和10000分别是年龄和收入吗？计算机只是拿到一堆数字。

| name | age  | income |
| ---- | ---- | ------ |
| 张三 | 25   | 10000  |
| 李四 | 36   | 50000  |
| 王五 | 49   | 30000  |
| ...  | ...  | ...    |

我们把X1看成是年龄，X2看成是收入，同时我们是知道y的，机器学习就是知道X,y的情况下解方程组调整出最优解的过程嘛。根据公式我们也可以发现y是两部分贡献之和，按常理来说，一开始并不知道两个部分谁更重要的情况下，可以想象为两部分对y的贡献是一样的即![img](https://www.itbaizhan.com/wiki/imgs/wps654.png)，如果![img](https://www.itbaizhan.com/wiki/imgs/wps655.png)，那么最终![img](https://www.itbaizhan.com/wiki/imgs/wps656.png)。

这样是不是就比较好理解为什么之前图里面右图为什么会画的![img](https://www.itbaizhan.com/wiki/imgs/wps657.png)这个轴比![img](https://www.itbaizhan.com/wiki/imgs/wps658.png)这个轴要长一些了。再思考一下，我们在梯度下降第1步的时候是不是所有的维度θ都是根据在期望μ为0方差σ为1的正太分布随机在0附近的，说白了就是一开始的![img](https://www.itbaizhan.com/wiki/imgs/wps659.png)和![img](https://www.itbaizhan.com/wiki/imgs/wps660.png)数值是差不多的。所以可以发现![img](https://www.itbaizhan.com/wiki/imgs/wps661.png)从初始值到目标位置![img](https://www.itbaizhan.com/wiki/imgs/wps662.png)的距离要远大于![img](https://www.itbaizhan.com/wiki/imgs/wps663.png)从初始值到目标位置![img](https://www.itbaizhan.com/wiki/imgs/wps664.png)。

再者，根据前面的![img](https://www.itbaizhan.com/wiki/imgs/wps665.png)，我们可以知道梯度是和X有关的，所以根据梯度公式![img](https://www.itbaizhan.com/wiki/imgs/wps666.png)可以推出![img](https://www.itbaizhan.com/wiki/imgs/wps667.png)。根据梯度下降公式![img](https://www.itbaizhan.com/wiki/imgs/wps668.png)，可以推出![img](https://www.itbaizhan.com/wiki/imgs/wps669.png)，也可以说每次调整![img](https://www.itbaizhan.com/wiki/imgs/wps670.png)的幅度要远小于![img](https://www.itbaizhan.com/wiki/imgs/wps671.png)的调整幅度。

总结，根据上面得到的两个结论，我们可以发现它们互相之间是矛盾的，意味着最后![img](https://www.itbaizhan.com/wiki/imgs/wps672.png)需要比![img](https://www.itbaizhan.com/wiki/imgs/wps673.png)更少的迭代次数就可以收敛，而我们要最终求得最优解，就必须每个维度θ都收敛才可以，所以会出现![img](https://www.itbaizhan.com/wiki/imgs/wps674.png)等待![img](https://www.itbaizhan.com/wiki/imgs/wps675.png)收敛的情况。讲到这里对应图大家应该可以理解为什么右图是先顺着![img](https://www.itbaizhan.com/wiki/imgs/wps676.png)的坐标轴往下走再往右走的原因了吧。

结论：归一化的一个目的是使得最终梯度下降的时候可以不同维度θ参数可以在接近的调整幅度上。这就好比社会主义，可以一小部分先富裕起来，至少loss整体可以下降，然后只是会等另外一部分人富裕起来。但是更好的是要实现共同富裕，最后每个人都不能落下，优化的步伐是一致的。

![img](https://www.itbaizhan.com/wiki/imgs/wps677.png)